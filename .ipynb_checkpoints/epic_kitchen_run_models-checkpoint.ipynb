{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.hub\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/epic-kitchens/action-models/archive/master.zip\" to /home/dimitri/.cache/torch/hub/master.zip\n",
      "Using cache found in /home/dimitri/.cache/torch/hub/epic-kitchens_action-models_master\n",
      "Using cache found in /home/dimitri/.cache/torch/hub/epic-kitchens_action-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Scale Temporal Relation Network Module in use ['8-frame relation', '7-frame relation', '6-frame relation', '5-frame relation', '4-frame relation', '3-frame relation', '2-frame relation']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/dimitri/.cache/torch/hub/epic-kitchens_action-models_master\n"
     ]
    }
   ],
   "source": [
    "repo = 'epic-kitchens/action-models'\n",
    "\n",
    "class_counts = (125, 352)\n",
    "segment_count = 8\n",
    "base_model = 'resnet50'\n",
    "\n",
    "tsn = torch.hub.load(repo, 'TSN', class_counts, segment_count, 'RGB',\n",
    "                     base_model=base_model, \n",
    "                     pretrained='epic-kitchens', force_reload=True)\n",
    "trn = torch.hub.load(repo, 'TRN', class_counts, segment_count, 'RGB',\n",
    "                     base_model=base_model, \n",
    "                     pretrained='epic-kitchens')\n",
    "mtrn = torch.hub.load(repo, 'MTRN', class_counts, segment_count, 'RGB',\n",
    "                     base_model=base_model, \n",
    "                      pretrained='epic-kitchens')\n",
    "tsm = torch.hub.load(repo, 'TSM', class_counts, segment_count, 'RGB',\n",
    "                     base_model=base_model, \n",
    "                     pretrained='epic-kitchens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms\n",
    "class GroupScale(object):\n",
    "    \"\"\" Rescales the input PIL.Image to the given 'size'.\n",
    "    'size' will be the size of the smaller edge.\n",
    "    For example, if height > width, then image will be\n",
    "    rescaled to (size * height / width, size)\n",
    "    size: size of the smaller edge\n",
    "    interpolation: Default: PIL.Image.BILINEAR\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
    "        self.worker = torchvision.transforms.Scale(size, interpolation)\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        return [self.worker(img) for img in img_group]\n",
    "       \n",
    "\n",
    "class GroupCenterCrop(object):\n",
    "    def __init__(self, size):\n",
    "        self.worker = torchvision.transforms.CenterCrop(size)\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        return [self.worker(img) for img in img_group]\n",
    "    \n",
    "\n",
    "class Stack(object):    \n",
    "    def __call__(self, img_group):\n",
    "        return np.array([np.array(i) for i in img_group])\n",
    "    \n",
    "\n",
    "class ToTorchFormatTensor(object):    \n",
    "    def __call__(self, pic):\n",
    "        img = torch.from_numpy(pic).permute(0, 3, 1, 2).contiguous()\n",
    "        return img.float().div(255) \n",
    "    \n",
    "\n",
    "class GroupNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        rep_mean = self.mean * (tensor.size()[0]//len(self.mean))\n",
    "        rep_std = self.std * (tensor.size()[0]//len(self.std))\n",
    "\n",
    "        # TODO: make efficient\n",
    "        for t, m, s in zip(tensor, rep_mean, rep_std):\n",
    "            t.sub_(m).div_(s)\n",
    "\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitri/.conda/envs/epic/lib/python3.7/site-packages/torchvision/transforms/transforms.py:256: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
     ]
    }
   ],
   "source": [
    "net = tsn\n",
    "scale_size = net.scale_size\n",
    "crop_size = net.input_size\n",
    "\n",
    "transform=torchvision.transforms.Compose([\n",
    "                       GroupScale(int(scale_size)),\n",
    "                       GroupCenterCrop(crop_size),\n",
    "                       Stack(),\n",
    "                       ToTorchFormatTensor(),\n",
    "                       GroupNormalize(net.input_mean, net.input_std),\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0.0, verb: 1, noun: 1\n",
      "time: 0.1, verb: 1, noun: 1\n",
      "time: 0.2, verb: 5, noun: 1\n",
      "time: 0.3, verb: 4, noun: 1\n",
      "time: 0.4, verb: 1, noun: 0\n",
      "time: 0.5, verb: 1, noun: 1\n",
      "time: 0.6, verb: 5, noun: 1\n",
      "time: 0.7, verb: 1, noun: 1\n",
      "time: 0.8, verb: 6, noun: 1\n",
      "time: 0.9, verb: 1, noun: 1\n",
      "time: 0.10, verb: 11, noun: 1\n",
      "time: 0.11, verb: 1, noun: 0\n",
      "time: 0.12, verb: 6, noun: 0\n",
      "time: 0.13, verb: 1, noun: 1\n",
      "time: 0.14, verb: 1, noun: 0\n",
      "time: 0.15, verb: 1, noun: 1\n",
      "time: 0.16, verb: 1, noun: 0\n",
      "time: 0.17, verb: 1, noun: 1\n",
      "time: 0.18, verb: 1, noun: 1\n",
      "time: 0.19, verb: 1, noun: 1\n",
      "time: 0.20, verb: 1, noun: 1\n",
      "time: 0.21, verb: 1, noun: 1\n",
      "time: 0.22, verb: 1, noun: 1\n",
      "time: 0.23, verb: 7, noun: 1\n",
      "time: 0.24, verb: 1, noun: 1\n",
      "time: 0.25, verb: 1, noun: 1\n",
      "time: 0.26, verb: 1, noun: 1\n",
      "time: 0.27, verb: 1, noun: 1\n",
      "time: 0.28, verb: 7, noun: 1\n",
      "time: 0.29, verb: 1, noun: 1\n",
      "time: 0.30, verb: 1, noun: 1\n",
      "time: 0.31, verb: 1, noun: 1\n",
      "time: 0.32, verb: 1, noun: 1\n",
      "time: 0.33, verb: 1, noun: 1\n",
      "time: 0.34, verb: 1, noun: 1\n",
      "time: 0.35, verb: 1, noun: 1\n",
      "time: 0.36, verb: 1, noun: 1\n",
      "time: 0.37, verb: 7, noun: 1\n",
      "time: 0.38, verb: 1, noun: 1\n",
      "time: 0.39, verb: 4, noun: 1\n",
      "time: 0.40, verb: 7, noun: 1\n",
      "time: 0.41, verb: 1, noun: 1\n",
      "time: 0.42, verb: 1, noun: 1\n",
      "time: 0.43, verb: 1, noun: 1\n",
      "time: 0.44, verb: 1, noun: 1\n",
      "time: 0.45, verb: 1, noun: 1\n",
      "time: 0.46, verb: 4, noun: 1\n",
      "time: 0.47, verb: 1, noun: 1\n",
      "time: 0.48, verb: 1, noun: 1\n",
      "time: 0.49, verb: 1, noun: 1\n",
      "time: 0.50, verb: 1, noun: 1\n",
      "time: 0.51, verb: 1, noun: 1\n",
      "time: 0.52, verb: 1, noun: 1\n",
      "time: 0.53, verb: 6, noun: 1\n",
      "time: 0.54, verb: 1, noun: 1\n",
      "time: 0.55, verb: 5, noun: 1\n",
      "time: 0.56, verb: 7, noun: 1\n",
      "time: 0.57, verb: 1, noun: 1\n",
      "time: 0.58, verb: 1, noun: 1\n",
      "time: 0.59, verb: 7, noun: 1\n",
      "time: 1.0, verb: 11, noun: 1\n",
      "time: 1.1, verb: 1, noun: 1\n",
      "time: 1.2, verb: 1, noun: 1\n",
      "time: 1.3, verb: 6, noun: 1\n",
      "time: 1.4, verb: 1, noun: 1\n",
      "time: 1.5, verb: 1, noun: 0\n",
      "time: 1.6, verb: 1, noun: 1\n",
      "time: 1.7, verb: 6, noun: 1\n",
      "time: 1.8, verb: 1, noun: 1\n",
      "time: 1.9, verb: 1, noun: 1\n",
      "time: 1.10, verb: 1, noun: 1\n",
      "time: 1.11, verb: 1, noun: 1\n",
      "time: 1.12, verb: 4, noun: 1\n"
     ]
    }
   ],
   "source": [
    "dir_name = os.path.join(\"../data\", \"P01_11\")\n",
    "\n",
    "segment_size = 30\n",
    "segment_num = 8\n",
    "\n",
    "step = 30\n",
    "frames_n = len(os.listdir(dir_name))\n",
    "for k in range(1, frames_n - segment_num * segment_size, step):\n",
    "    image_paths = [os.path.join(dir_name, \"frame_{}.jpg\".format(str(k + i * segment_size).zfill(10))) for i in range(segment_num)]\n",
    "    images = [Image.open(image_p).convert('RGB') for image_p in image_paths]\n",
    "    inputs = transform(images)\n",
    "    features = tsn.features(inputs)\n",
    "    verb_logits, noun_logits = tsn.logits(features)\n",
    "    print(\"time: {}.{}, verb: {}, noun: {}\".format(\n",
    "          (k // segment_size) // 60,\n",
    "          (k // segment_size) % 60, \n",
    "          torch.argmax(noun_logits).item(),\n",
    "          torch.argmax(verb_logits).item()\n",
    "         ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
